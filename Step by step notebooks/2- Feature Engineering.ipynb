{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2- Feature Engineering","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMGVKBpLIVXROxxrC5ufrg/"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9j3JT5_VfyRD","colab_type":"text"},"source":["# First look at the dataset"]},{"cell_type":"markdown","metadata":{"id":"4WFTfJPeYiu1","colab_type":"text"},"source":["The dataset contains 49 variables. There is **no independent variable**. I will implement it.\n","\n","First I tried to test a very simple model using those features: \n","- *winner_rank_points*\n","- *loser_rank_points*\n","\n","I chose *rank_points* features over *rank* features because there can be a either small or huge difference of rank points between for example the 2nd ATP player and the 3rd ATP player. \n","<br>\n","rank_points are more meaningfull than the rank of a player. \n","<br>\n","I didn't pick both features to avoid **multicollinearity** that would weaken my model.\n","<br><br>\n","The features *winner_rank_points* and *loser_rank_points* are related to the player that will either win or lose the match. \n","\n","To avoid **target leakage**, I renamed those features as *player_1_points* and *player_2_points* and **added the independent variable** *player_1_wins*. Then *player_1_wins* would always be 1. So I found 2 option to solve that:\n","\n","1. Add to the dataset its inverse (switch *player_1_points* and *player_2_points* and set *player_1_wins* = 0)\n","2. Inverse 50% of the actual dataset. The training dataset size will remain the same.\n","\n","Option 1 may be not very good as it would multiply by 2 the size of the training set. But it might yield better results than option 2. So I decided to test both options.\n"]},{"cell_type":"code","metadata":{"id":"dooe0NxJQxXH","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error\n","\n","def inverseDataset(dataset_input):\n","    '''inverse dataset - for option 1'''\n","    inversed_dataset = pd.DataFrame()\n","    inversed_dataset[\"player_1_points\"] = dataset_input[\"player_2_points\"]\n","    inversed_dataset[\"player_2_points\"] = dataset_input[\"player_1_points\"]\n","    inversed_dataset[\"player_1_wins\"] = 0\n","    return inversed_dataset\n","\n","def inverseHalfDataset(dataset_input):\n","    '''inverse 50% of the dataset - for option 2'''\n","    inversed_dataset = pd.DataFrame()\n","    inversed_dataset[\"player_1_points\"] = np.where(dataset_input.index % 2 == 0, dataset_input[\"player_1_points\"] , dataset_input[\"player_2_points\"])\n","    inversed_dataset[\"player_2_points\"] = np.where(dataset_input.index % 2 == 0, dataset_input[\"player_2_points\"] , dataset_input[\"player_1_points\"])\n","    inversed_dataset[\"player_1_wins\"] = np.where(dataset_input.index % 2 == 0, 1, 0)\n","    return inversed_dataset    \n","\n","# Read the data\n","list_datasets = []\n","for year in range(2000, 2010):\n","    dataset = pd.read_csv(\"https://raw.githubusercontent.com/davy-datascience/tennis-prediction/master/datasets/atp_matches_{}.csv\".format(year))\n","    list_datasets.append(dataset)\n","\n","full_dataset = pd.concat(list_datasets)\n","\n","features = [\"winner_rank_points\", \"loser_rank_points\"]\n","\n","dataset = full_dataset[features]\n","\n","#drop rows with null value\n","dataset = dataset.dropna()\n","\n","dataset = dataset.rename(columns={'winner_rank_points': 'player_1_points', 'loser_rank_points': 'player_2_points'})\n","dataset[\"player_1_wins\"] = 1\n","\n","### OPTION 1\n","# Separate the dataset into a training set and a test set\n","train, test = train_test_split(dataset, test_size = 0.2)\n","    \n","inversed_train = inverseDataset(train)\n","train = pd.concat([train, inversed_train])\n","\n","X_train = train[[\"player_1_points\", \"player_2_points\"]]\n","y_train = train.player_1_wins\n","X_test = test[[\"player_1_points\", \"player_2_points\"]]\n","y_test = test.player_1_wins\n","\n","from sklearn.linear_model import LogisticRegression\n","classifier = LogisticRegression(solver=\"liblinear\")\n","classifier.fit(X_train, y_train)\n","\n","# Predict\n","y_pred = pd.Series(classifier.predict(X_test), index = y_test.index)\n","mae = mean_absolute_error(y_pred, y_test)\n","print(\"MAE using option 1: {}\".format(mae))\n","\n","\n","### OPTION 2\n","# Separate the dataset into a training set and a test set\n","train, test = train_test_split(dataset, test_size = 0.2)\n","train = inverseHalfDataset(train)\n","\n","X_train = train[[\"player_1_points\", \"player_2_points\"]]\n","y_train = train.player_1_wins\n","X_test = test[[\"player_1_points\", \"player_2_points\"]]\n","y_test = test.player_1_wins\n","\n","# Predict\n","y_pred = pd.Series(classifier.predict(X_test), index = y_test.index)\n","mae = mean_absolute_error(y_pred, y_test)\n","print(\"MAE using option 2: {}\".format(mae))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DNVvavaTdxd8","colab_type":"text"},"source":["MAE (mean absolute error) for option 2 is almost equal and even a bit lower than for option 1. So option 1 doesn't get better results than option 1, it only increase the dataset size. Therefore I kept option 2 method.\n","<br><br>\n","Other variables are related to the winner or the loser of the match (as *winner_age*, *loser_age*, ...)\n","\n","I will consider those variables in the *inverseHalfDataset* method."]},{"cell_type":"markdown","metadata":{"id":"2MhbB5gBmCpI","colab_type":"text"},"source":["# Feature importance"]},{"cell_type":"markdown","metadata":{"id":"BOCtsudBoUVw","colab_type":"text"},"source":["Before going further I will use PermutationImportance from sklearn to detect which features seems the most important at first sight. As there is a lot of variables, permutation importance will help"]}]}