{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2- Data Preparation","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyODomQ90DO/HKT0KrspgrOm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_N1cdTiBcQgh"},"source":["# First look at the dataset"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4WFTfJPeYiu1"},"source":["The dataset contains 49 variables. There is **no independent variable**. So I will implement it.\n","\n","First I tried to test a very simple model using those features: \n","- *winner_rank_points*\n","- *loser_rank_points*\n","\n","I chose *rank_points* features over *rank* features because there can be a either small or huge difference of rank points between for example the 2nd ATP player and the 3rd ATP player. \n","<br>\n","rank_points are more meaningfull than the rank of a player. \n","<br>\n","I didn't pick both features to avoid **multicollinearity** that would weaken my model.\n","<br><br>\n","The features *winner_rank_points* and *loser_rank_points* are related to the player that will either win or lose the match. \n","\n","To avoid **target leakage**, I renamed those features as *p1_points* and *p2_points* and **added the independent variable** *p1_wins*. Then *p1_wins* would always be 1. So I found 2 option to solve the fact that the independent variable has always the same value:\n","\n","1. Add to the dataset its inverse (switch *p1_points* and *p2_points* and set *p1_wins* = 0)\n","2. Inverse 50% of the actual dataset. The training dataset size will remain the same.\n","\n","Option 1 may be not very good as it would multiply by 2 the size of the training set. But it might yield better results than option 2. So I decided to test both options.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dooe0NxJQxXH","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"elapsed":8860,"status":"ok","timestamp":1595257215474,"user":{"displayName":"Davy Azoulay","photoUrl":"","userId":"11100852291658680302"},"user_tz":-120},"outputId":"7813442e-af66-4acf-f6b1-17b34e638302"},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error\n","\n","def inverseDataset(dataset_input):\n","    '''inverse dataset - for option 1'''\n","    inversed_dataset = pd.DataFrame()\n","    inversed_dataset[\"p1_points\"] = dataset_input[\"p2_points\"]\n","    inversed_dataset[\"p2_points\"] = dataset_input[\"p1_points\"]\n","    inversed_dataset[\"p1_wins\"] = 0\n","    return inversed_dataset\n","\n","def inverseHalfDataset(dataset_input):\n","    '''inverse 50% of the dataset - for option 2'''\n","    inversed_dataset = pd.DataFrame()\n","    inversed_dataset[\"p1_points\"] = np.where(dataset_input.index % 2 == 0, dataset_input[\"p1_points\"] , dataset_input[\"p2_points\"])\n","    inversed_dataset[\"p2_points\"] = np.where(dataset_input.index % 2 == 0, dataset_input[\"p2_points\"] , dataset_input[\"p1_points\"])\n","    inversed_dataset[\"p1_wins\"] = np.where(dataset_input.index % 2 == 0, 1, 0)\n","    return inversed_dataset    \n","\n","# Read the data\n","list_datasets = []\n","for year in range(2000, 2010):\n","    dataset = pd.read_csv(\"https://raw.githubusercontent.com/davy-datascience/tennis-prediction/master/datasets/atp_matches_{}.csv\".format(year))\n","    list_datasets.append(dataset)\n","\n","full_dataset = pd.concat(list_datasets)\n","\n","features = [\"winner_rank_points\", \"loser_rank_points\"]\n","\n","dataset = full_dataset[features]\n","\n","#drop rows with null value\n","dataset = dataset.dropna()\n","\n","dataset = dataset.rename(columns={'winner_rank_points': 'p1_points', 'loser_rank_points': 'p2_points'})\n","dataset[\"p1_wins\"] = 1\n","\n","### OPTION 1\n","# Separate the dataset into a training set and a test set\n","train, test = train_test_split(dataset, test_size = 0.2)\n","    \n","inversed_train = inverseDataset(train)\n","train = pd.concat([train, inversed_train])\n","\n","X_train = train[[\"p1_points\", \"p2_points\"]]\n","y_train = train.p1_wins\n","X_test = test[[\"p1_points\", \"p2_points\"]]\n","y_test = test.p1_wins\n","\n","from sklearn.linear_model import LogisticRegression\n","classifier = LogisticRegression(solver=\"liblinear\")\n","classifier.fit(X_train, y_train)\n","\n","# Predict\n","y_pred = pd.Series(classifier.predict(X_test), index = y_test.index)\n","mae = mean_absolute_error(y_pred, y_test)\n","print(\"MAE using option 1: {}\".format(mae))\n","\n","\n","### OPTION 2\n","# Separate the dataset into a training set and a test set\n","train, test = train_test_split(dataset, test_size = 0.2)\n","train = inverseHalfDataset(train)\n","\n","X_train = train[[\"p1_points\", \"p2_points\"]]\n","y_train = train.p1_wins\n","X_test = test[[\"p1_points\", \"p2_points\"]]\n","y_test = test.p1_wins\n","\n","# Predict\n","y_pred = pd.Series(classifier.predict(X_test), index = y_test.index)\n","mae = mean_absolute_error(y_pred, y_test)\n","print(\"MAE using option 2: {}\".format(mae))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MAE using option 1: 0.3478676002546149\n","MAE using option 2: 0.34707192870782944\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DNVvavaTdxd8"},"source":["MAE (mean absolute error) for option 2 is almost equal and even a bit lower than for option 1. So option 1 doesn't yield better results than option 1, it only increase the dataset size. Therefore I kept option 2 methodology.\n","<br><br>\n","Other variables are related to the winner or the loser of the match (as *winner_age*, *loser_age*, ...)\n","\n","I will rename those variables by adding \"*p1_*\" and \"*p2_*\" prefixes and consider them in the *inverseHalfDataset* method."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2MhbB5gBmCpI"},"source":["# Indentify variables causing target leakage "]},{"cell_type":"markdown","metadata":{"id":"vW6XtGlHdQh9","colab_type":"text"},"source":["On this dataset, several data is data that is not available before the match ends. This data should be excluded before modeling. However this data would be usefull to build new features as the percentage of first-serve points won in the last 3 matches, the number of break points faced in the last 3 matches, ...\n","\n","Variables that are not available before the moment I should make predictions are:\n","\n","\"*p1_ace*\", \"*p1_df*\", \"*p1_svpt*\", \"*p1_1stIn*\", \"*p1_1stWon*\", \"*p1_2ndWon*\", \"*p1_SvGms*\", \"*p1_SvGms*\", \"*p1_bpSaved*\", \"*p1_bpFaced*\", \"*p2_ace*\", \"*p2_df*\", \"*p2_svpt*\", \"*p2_1stIn*\", \"*p2_1stWon*\", \"*p2_2ndWon*\", \"*p2_SvGms*\", \"*p2_SvGms*\", \"*p2_bpSaved*\", \"*p2_bpFaced*\"\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9j3JT5_VfyRD"},"source":["# Feature Engineering\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YXzNWnPxpKMe","colab_type":"text"},"source":["I added some features that will be usefull for further pre-processing."]},{"cell_type":"code","metadata":{"id":"x9xl6GSsMLzH","colab_type":"code","colab":{}},"source":["@numba.vectorize\n","def divideWithNumba(a, b):\n","''' Divide one column by an other column of a dataframe with increased performance thanks to vectorization '''\n","    return a / b\n","\n","def getBpSavedRatio(a, b):\n","    ''' Divide break point saved by break point faced, if no break point faced consider as 1: max ratio'''\n","    return 1 if b == 0 else (a/b)\n","\n","dataset[\"p1_ace_ratio\"] = divideWithNumba(dataset[\"p1_ace\"].to_numpy(), dataset[\"p1_svpt\"].to_numpy())\n","dataset[\"p2_ace_ratio\"] = divideWithNumba(dataset[\"p2_ace\"].to_numpy(), dataset[\"p2_svpt\"].to_numpy())\n","dataset[\"p1_df_ratio\"] = divideWithNumba(dataset[\"p1_df\"].to_numpy(), dataset[\"p1_svpt\"].to_numpy())\n","dataset[\"p2_df_ratio\"] = divideWithNumba(dataset[\"p2_df\"].to_numpy(), dataset[\"p2_svpt\"].to_numpy())\n","dataset[\"p1_1stIn_ratio\"] = divideWithNumba(dataset[\"p1_1stIn\"].to_numpy(), dataset[\"p1_svpt\"].to_numpy())\n","dataset[\"p2_1stIn_ratio\"] = divideWithNumba(dataset[\"p2_1stIn\"].to_numpy(), dataset[\"p2_svpt\"].to_numpy())\n","dataset[\"p1_1stWon_ratio\"] = divideWithNumba(dataset[\"p1_1stWon\"].to_numpy(), dataset[\"p1_svpt\"].to_numpy())\n","dataset[\"p2_1stWon_ratio\"] = divideWithNumba(dataset[\"p2_1stWon\"].to_numpy(), dataset[\"p2_svpt\"].to_numpy())\n","dataset[\"p1_2ndWon_ratio\"] = divideWithNumba(dataset[\"p1_2ndWon\"].to_numpy(), dataset[\"p1_svpt\"].to_numpy())\n","dataset[\"p2_2ndWon_ratio\"] = divideWithNumba(dataset[\"p2_2ndWon\"].to_numpy(), dataset[\"p2_svpt\"].to_numpy())\n","dataset[\"p1_bpFaced_ratio\"] = divideWithNumba(dataset[\"p1_bpFaced\"].to_numpy(), dataset[\"p1_SvGms\"].to_numpy()) # Break points Faced per return-game\n","dataset[\"p2_bpFaced_ratio\"] = divideWithNumba(dataset[\"p2_bpFaced\"].to_numpy(), dataset[\"p2_SvGms\"].to_numpy()) # Break points Faced per return-game\n","dataset[\"p1_bpSaved_ratio\"] = [getBpSavedRatio(row[0], row[1]) for row in dataset[[\"p1_bpSaved\", \"p1_bpFaced\"]].to_numpy()]       \n","dataset[\"p2_bpSaved_ratio\"] = [getBpSavedRatio(row[0], row[1]) for row in dataset[[\"p2_bpSaved\", \"p2_bpFaced\"]].to_numpy()]       \n","dataset['tourney_date'] = pd.to_datetime(dataset['tourney_date'], format=\"%Y%m%d\") "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a80wor5JVr4F","colab_type":"text"},"source":["Those new feature help me create the following features"]},{"cell_type":"code","metadata":{"id":"Z3HVwpP5WRCW","colab_type":"code","colab":{}},"source":["def getPreviousResults(player_results, index, p1_id, p2_id):\n","    results_p1 = player_results[p1_id]\n","    prev_res_p1 = pd.DataFrame([results_p1.loc[i] for i in results_p1.index if i < index])\n","    \n","    results_p2 = player_results[p2_id]\n","    prev_res_p2 = pd.DataFrame([results_p2.loc[i] for i in results_p2.index if i < index])\n","    \n","    (\n","     p1_ace_ratio_last3, p2_ace_ratio_last3, p1_df_ratio_last3, p2_df_ratio_last3, p1_1stIn_ratio_last3, \n","     p2_1stIn_ratio_last3, p1_1stWon_ratio_last3, p2_1stWon_ratio_last3, p1_2ndWon_ratio_last3, p2_2ndWon_ratio_last3,\n","     p1_bpSaved_ratio_last3, p2_bpSaved_ratio_last3, p1_bpFaced_ratio_last3, p2_bpFaced_ratio_last3\n","     ) = (None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n","    \n","    if len(prev_res_p1) > 0 :\n","        p1_ace_ratio_last3 = prev_res_p1[\"p1_ace_ratio\"].tail(3).mean()\n","        p1_df_ratio_last3 = prev_res_p1[\"p1_df_ratio\"].tail(3).mean()\n","        p1_1stIn_ratio_last3 = prev_res_p1[\"p1_1stIn_ratio\"].tail(3).mean()\n","        p1_1stWon_ratio_last3 = prev_res_p1[\"p1_1stWon_ratio\"].tail(3).mean()\n","        p1_2ndWon_ratio_last3 = prev_res_p1[\"p1_2ndWon_ratio\"].tail(3).mean()\n","        p1_bpSaved_ratio_last3 = prev_res_p1[\"p1_bpSaved_ratio\"].tail(3).mean()        \n","        p1_bpFaced_ratio_last3 = prev_res_p1[\"p1_bpFaced\"].tail(3).mean()  \n","        \n","    if len(prev_res_p2) > 0 :\n","        p2_ace_ratio_last3 = prev_res_p2[\"p2_ace_ratio\"].tail(3).mean()\n","        p2_df_ratio_last3 = prev_res_p2[\"p2_df_ratio\"].tail(3).mean()\n","        p2_1stIn_ratio_last3 = prev_res_p2[\"p2_1stIn_ratio\"].tail(3).mean()\n","        p2_1stWon_ratio_last3 = prev_res_p2[\"p2_1stWon_ratio\"].tail(3).mean()\n","        p2_2ndWon_ratio_last3 = prev_res_p2[\"p2_2ndWon_ratio\"].tail(3).mean()\n","        p2_bpSaved_ratio_last3 = prev_res_p2[\"p2_bpSaved_ratio\"].tail(3).mean()\n","        p2_bpFaced_ratio_last3 = prev_res_p2[\"p2_bpFaced\"].tail(3).mean()  \n","    \n","    return (p1_ace_ratio_last3, p2_ace_ratio_last3, p1_df_ratio_last3, p2_df_ratio_last3, \n","            p1_1stIn_ratio_last3, p2_1stIn_ratio_last3, p1_1stWon_ratio_last3, p2_1stWon_ratio_last3, \n","            p1_2ndWon_ratio_last3, p2_2ndWon_ratio_last3, p1_bpSaved_ratio_last3, p2_bpSaved_ratio_last3,\n","            p1_bpFaced_ratio_last3, p2_bpFaced_ratio_last3)\n","    \n","\n","\n","player_results = {}\n","\n","for pid in player_ids:\n","    '''idx = np.where((dataset[\"p1_id\"] == pid) | (dataset[\"p2_id\"] == pid))\n","    all_matchs = dataset.iloc[idx[0]]'''\n","    all_matchs = dataset.loc[(dataset[\"p1_id\"] == pid) | (dataset[\"p2_id\"] == pid)]\n","    all_wins = all_matchs[all_matchs[\"p1_id\"] == pid]\n","    all_lost = all_matchs[all_matchs[\"p2_id\"] == pid]\n","    \n","    player_results[pid]= pd.concat([all_wins, inverseDataset(all_lost)]).sort_index()\n","\n","print(\"--- %s seconds ---\" % (time.time() - start_time))\n","\n","\n","start_time = time.time()\n","results = [getPreviousResults(player_results, index, ids[0], ids[1]) for index, ids in dataset[[\"p1_id\", \"p2_id\"]].iterrows()]\n","print(\"--- %s seconds ---\" % (time.time() - start_time))\n","\n","\n","dataset[\"p1_ace_ratio_last3\"] = [result[0] for result in results]\n","dataset[\"p2_ace_ratio_last3\"] = [result[1] for result in results]\n","dataset[\"p1_df_ratio_last3\"] = [result[2] for result in results]\n","dataset[\"p2_df_ratio_last3\"] = [result[3] for result in results]\n","dataset[\"p1_1stIn_ratio_last3\"] = [result[4] for result in results]\n","dataset[\"p2_1stIn_ratio_last3\"] = [result[5] for result in results]\n","dataset[\"p1_1stWon_ratio_last3\"] = [result[6] for result in results]\n","dataset[\"p2_1stWon_ratio_last3\"] = [result[7] for result in results]\n","dataset[\"p1_2ndWon_ratio_last3\"] = [result[8] for result in results]\n","dataset[\"p2_2ndWon_ratio_last3\"] = [result[9] for result in results]\n","dataset[\"p1_bpSaved_ratio_last3\"] = [result[10] for result in results]\n","dataset[\"p2_bpSaved_ratio_last3\"] = [result[11] for result in results]\n","dataset[\"p1_bpFaced_ratio_last3\"] = [result[12] for result in results]\n","dataset[\"p2_bpFaced_ratio_last3\"] = [result[13] for result in results]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vSsqQFq7DGSn"},"source":["# Feature Importance"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BOCtsudBoUVw"},"source":["I am using PermutationImportance from sklearn to detect which features seems the most important at first sight.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-A2qQaVTdmsk","colab":{}},"source":["!pip install eli5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KpPmw-V1ctXC","colab":{}},"source":["# Separate the dataset into a training set and a test set\n","X_train, X_test, y_train, y_test= train_test_split(X, y, test_size = 0.2, shuffle=False)\n","\n","# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n","categorical_cols = [cname for cname in X_train.columns if\n","                    X_train[cname].nunique() < 10 and \n","                    X_train[cname].dtype == \"object\"]\n","\n","# Select numerical columns\n","numerical_cols = [cname for cname in X_train.columns if \n","                X_train[cname].dtype in ['int64', 'float64']]\n","\n","# Keep selected columns only\n","my_cols = categorical_cols + numerical_cols\n","X_train = X_train[my_cols].copy()\n","X_test = X_test[my_cols].copy()\n","\n","columnTransformer = ColumnTransformer([('encoder', OneHotEncoder(), categorical_cols)], remainder='passthrough')\n","#remainder='passthrough' : keep other columns (default:'drop')\n","\n","transformed_data = np.array(columnTransformer.fit_transform(X_train), dtype = np.str)\n","transformed_data_test = np.array(columnTransformer.transform(X_test), dtype = np.str)\n","\n","X_train = pd.DataFrame(transformed_data, columns=get_ct_feature_names(columnTransformer))\n","X_test = pd.DataFrame(transformed_data_test, columns=get_ct_feature_names(columnTransformer))\n","\n","# Fill in the lines below: imputation\n","my_imputer = SimpleImputer()\n","imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n","imputed_X_test = pd.DataFrame(my_imputer.transform(X_test))\n","\n","# Fill in the lines below: imputation removed column names; put them back\n","imputed_X_train.columns = X_train.columns\n","imputed_X_test.columns = X_test.columns\n","\n","X_train = imputed_X_train\n","X_test = imputed_X_test\n","\n","my_model = LogisticRegression()\n","my_model.fit(X_train, y_train)\n","\n","perm = PermutationImportance(my_model).fit(X_test, y_test)\n","eli5.show_weights(perm, feature_names = X_test.columns.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2N_Cdlm6VfnH","colab_type":"text"},"source":["![feature importance](https://raw.githubusercontent.com/davy-datascience/tennis-prediction/master/img/feature_importance_0.PNG)"]},{"cell_type":"markdown","metadata":{"id":"bXh_R5V31f6S","colab_type":"text"},"source":["# Suggested improvement"]},{"cell_type":"markdown","metadata":{"id":"C3L0evet1n0j","colab_type":"text"},"source":["-"]}]}